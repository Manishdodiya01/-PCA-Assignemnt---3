{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c26b93d-162e-4e7e-bd6e-1f32b73faaad",
   "metadata": {},
   "source": [
    "# Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02aef8-006d-498f-99a8-915b39e7401e",
   "metadata": {},
   "source": [
    " Eigenvalues and Eigenvectors:\n",
    "\n",
    "In linear algebra, given a square matrix \n",
    "�\n",
    "A, a non-zero vector \n",
    "�\n",
    "v is an eigenvector of \n",
    "�\n",
    "A if when \n",
    "�\n",
    "A is applied to \n",
    "�\n",
    "v, the result is a scalar multiple of \n",
    "�\n",
    "v. This scalar multiple is known as the eigenvalue corresponding to the eigenvector.\n",
    "\n",
    "Mathematically, if \n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "Av=λv, where \n",
    "�\n",
    "λ is the eigenvalue, then \n",
    "�\n",
    "v is an eigenvector of \n",
    "�\n",
    "A associated with eigenvalue \n",
    "�\n",
    "λ.\n",
    "\n",
    "Eigen-Decomposition:\n",
    "\n",
    "The eigen-decomposition is a factorization of a matrix \n",
    "�\n",
    "A into a canonical form, whereby \n",
    "�\n",
    "A is represented in terms of its eigenvectors and eigenvalues.\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "A=PDP \n",
    "−1\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "P is a matrix whose columns are the eigenvectors of \n",
    "�\n",
    "A.\n",
    "�\n",
    "D is a diagonal matrix whose entries are the eigenvalues corresponding to the respective eigenvectors.\n",
    "Example:\n",
    "\n",
    "Consider the matrix \n",
    "�\n",
    "A and the vectors \n",
    "�\n",
    "v below:\n",
    "\n",
    "�\n",
    "=\n",
    "[\n",
    "4\n",
    "1\n",
    "2\n",
    "3\n",
    "]\n",
    "A=[ \n",
    "4\n",
    "2\n",
    "​\n",
    "  \n",
    "1\n",
    "3\n",
    "​\n",
    " ]\n",
    "�\n",
    "1\n",
    "=\n",
    "[\n",
    "1\n",
    "1\n",
    "]\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "[\n",
    "1\n",
    "−\n",
    "2\n",
    "]\n",
    "v \n",
    "1\n",
    "​\n",
    " =[ \n",
    "1\n",
    "1\n",
    "​\n",
    " ],v \n",
    "2\n",
    "​\n",
    " =[ \n",
    "1\n",
    "−2\n",
    "​\n",
    " ]\n",
    "\n",
    "The eigenvalues and eigenvectors can be calculated:\n",
    "\n",
    "For \n",
    "�\n",
    "1\n",
    "v \n",
    "1\n",
    "​\n",
    " :\n",
    "�\n",
    "�\n",
    "1\n",
    "=\n",
    "[\n",
    "4\n",
    "1\n",
    "2\n",
    "3\n",
    "]\n",
    "[\n",
    "1\n",
    "1\n",
    "]\n",
    "=\n",
    "[\n",
    "5\n",
    "5\n",
    "]\n",
    "=\n",
    "5\n",
    "[\n",
    "1\n",
    "1\n",
    "]\n",
    "=\n",
    "5\n",
    "�\n",
    "1\n",
    "Av \n",
    "1\n",
    "​\n",
    " =[ \n",
    "4\n",
    "2\n",
    "​\n",
    "  \n",
    "1\n",
    "3\n",
    "​\n",
    " ][ \n",
    "1\n",
    "1\n",
    "​\n",
    " ]=[ \n",
    "5\n",
    "5\n",
    "​\n",
    " ]=5[ \n",
    "1\n",
    "1\n",
    "​\n",
    " ]=5v \n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "So, \n",
    "�\n",
    "1\n",
    "v \n",
    "1\n",
    "​\n",
    "  is an eigenvector of \n",
    "�\n",
    "A with eigenvalue \n",
    "�\n",
    "1\n",
    "=\n",
    "5\n",
    "λ \n",
    "1\n",
    "​\n",
    " =5.\n",
    "\n",
    "For \n",
    "�\n",
    "2\n",
    "v \n",
    "2\n",
    "​\n",
    " :\n",
    "�\n",
    "�\n",
    "2\n",
    "=\n",
    "[\n",
    "4\n",
    "1\n",
    "2\n",
    "3\n",
    "]\n",
    "[\n",
    "1\n",
    "−\n",
    "2\n",
    "]\n",
    "=\n",
    "[\n",
    "3\n",
    "−\n",
    "4\n",
    "]\n",
    "=\n",
    "−\n",
    "1\n",
    "[\n",
    "1\n",
    "−\n",
    "2\n",
    "]\n",
    "=\n",
    "−\n",
    "1\n",
    "�\n",
    "2\n",
    "Av \n",
    "2\n",
    "​\n",
    " =[ \n",
    "4\n",
    "2\n",
    "​\n",
    "  \n",
    "1\n",
    "3\n",
    "​\n",
    " ][ \n",
    "1\n",
    "−2\n",
    "​\n",
    " ]=[ \n",
    "3\n",
    "−4\n",
    "​\n",
    " ]=−1[ \n",
    "1\n",
    "−2\n",
    "​\n",
    " ]=−1v \n",
    "2\n",
    "​\n",
    " \n",
    "\n",
    "So, \n",
    "�\n",
    "2\n",
    "v \n",
    "2\n",
    "​\n",
    "  is an eigenvector of \n",
    "�\n",
    "A with eigenvalue \n",
    "�\n",
    "2\n",
    "=\n",
    "−\n",
    "1\n",
    "λ \n",
    "2\n",
    "​\n",
    " =−1.\n",
    "\n",
    "Therefore, \n",
    "�\n",
    "=\n",
    "[\n",
    "1\n",
    "1\n",
    "1\n",
    "−\n",
    "2\n",
    "]\n",
    "P=[ \n",
    "1\n",
    "1\n",
    "​\n",
    "  \n",
    "1\n",
    "−2\n",
    "​\n",
    " ] (matrix of eigenvectors) and \n",
    "�\n",
    "=\n",
    "[\n",
    "5\n",
    "0\n",
    "0\n",
    "−\n",
    "1\n",
    "]\n",
    "D=[ \n",
    "5\n",
    "0\n",
    "​\n",
    "  \n",
    "0\n",
    "−1\n",
    "​\n",
    " ] (diagonal matrix of eigenvalues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e633f3-f103-4abc-9f27-98f406420299",
   "metadata": {},
   "source": [
    "# Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293134dd-0564-4410-a645-2ea42eb0335e",
   "metadata": {},
   "source": [
    "Significance of Eigen-Decomposition:\n",
    "\n",
    "Eigen-decomposition is significant in linear algebra because it provides a way to represent a matrix in a form that highlights its underlying structure. It allows for easier analysis of the properties and behavior of the matrix.\n",
    "\n",
    "Additionally, eigen-decomposition is fundamental in various areas including physics, engineering, statistics, and computer science. For example, in physics, eigenvalues and eigenvectors are used to solve systems of linear differential equations, and in computer science, they play a crucial role in algorithms for tasks like image processing and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d15451-73cb-4319-bb58-fec728535849",
   "metadata": {},
   "source": [
    "# Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0cc52-206e-4a00-8238-09893913bb02",
   "metadata": {},
   "source": [
    " Conditions for Diagonalizability using Eigen-Decomposition:\n",
    "\n",
    "A square matrix \n",
    "�\n",
    "A is diagonalizable if and only if it has \n",
    "�\n",
    "n linearly independent eigenvectors, where \n",
    "�\n",
    "n is the size of the matrix.\n",
    "\n",
    "Proof (Sketch):\n",
    "\n",
    "If a matrix \n",
    "�\n",
    "A has \n",
    "�\n",
    "n linearly independent eigenvectors, then the matrix \n",
    "�\n",
    "P in the eigen-decomposition will have \n",
    "�\n",
    "n linearly independent columns. In this case, \n",
    "�\n",
    "P will be invertible.\n",
    "\n",
    "If \n",
    "�\n",
    "A is diagonalizable, then \n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "A=PDP \n",
    "−1\n",
    "  for some invertible matrix \n",
    "�\n",
    "P and diagonal matrix \n",
    "�\n",
    "D. Rearranging, we have \n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "AP=PD.\n",
    "\n",
    "Multiplying both sides by \n",
    "�\n",
    "−\n",
    "1\n",
    "P \n",
    "−1\n",
    "  on the right, we get \n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "⇒\n",
    "�\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "⇒\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "AP=PD⇒APP \n",
    "−1\n",
    " =PDP \n",
    "−1\n",
    " ⇒A=PDP \n",
    "−1\n",
    " .\n",
    "\n",
    "This shows that \n",
    "�\n",
    "A can be diagonalized if and only if \n",
    "�\n",
    "P is invertible, which happens if and only if \n",
    "�\n",
    "A has \n",
    "�\n",
    "n linearly independent eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f1f52-f4ec-4e9f-8348-87f26e92943b",
   "metadata": {},
   "source": [
    "# Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8b977-c17d-4c1f-8dfa-055b7e5e9190",
   "metadata": {},
   "source": [
    "Significance of the Spectral Theorem and its Relation to Diagonalizability:\n",
    "\n",
    "The Spectral Theorem states that for a symmetric (or Hermitian in the complex case) matrix, there exists an orthonormal basis of eigenvectors that can be used to diagonalize the matrix. In other words, if a matrix is symmetric, it can be decomposed into a diagonal matrix by a similarity transformation using its eigenvectors.\n",
    "\n",
    "This is highly significant because it provides a powerful tool for analyzing and understanding symmetric matrices. It also has important implications in various fields, including physics, where symmetric matrices often arise in the context of quadratic forms and energy states.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider the symmetric matrix:\n",
    "\n",
    "�\n",
    "=\n",
    "[\n",
    "4\n",
    "1\n",
    "1\n",
    "3\n",
    "]\n",
    "A=[ \n",
    "4\n",
    "1\n",
    "​\n",
    "  \n",
    "1\n",
    "3\n",
    "​\n",
    " ]\n",
    "\n",
    "The eigenvalues and eigenvectors can be calculated as follows:\n",
    "\n",
    "Eigenvalues:\n",
    "\n",
    "The characteristic polynomial of \n",
    "�\n",
    "A is \n",
    "(\n",
    "4\n",
    "−\n",
    "�\n",
    ")\n",
    "(\n",
    "3\n",
    "−\n",
    "�\n",
    ")\n",
    "−\n",
    "1\n",
    "=\n",
    "�\n",
    "2\n",
    "−\n",
    "7\n",
    "�\n",
    "+\n",
    "11\n",
    "(4−λ)(3−λ)−1=λ \n",
    "2\n",
    " −7λ+11, which has roots \n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    "λ \n",
    "1\n",
    "​\n",
    " =3 and \n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "λ \n",
    "2\n",
    "​\n",
    " =4.\n",
    "Eigenvectors:\n",
    "\n",
    "For \n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    "λ \n",
    "1\n",
    "​\n",
    " =3, we have \n",
    "(\n",
    "�\n",
    "−\n",
    "3\n",
    "�\n",
    ")\n",
    "�\n",
    "=\n",
    "0\n",
    "(A−3I)v=0, where \n",
    "�\n",
    "v is the eigenvector corresponding to \n",
    "�\n",
    "1\n",
    "λ \n",
    "1\n",
    "​\n",
    " . Solving, we get \n",
    "�\n",
    "1\n",
    "=\n",
    "[\n",
    "1\n",
    "−\n",
    "1\n",
    "]\n",
    "v \n",
    "1\n",
    "​\n",
    " =[ \n",
    "1\n",
    "−1\n",
    "​\n",
    " ].\n",
    "\n",
    "For \n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "λ \n",
    "2\n",
    "​\n",
    " =4, similarly, we find \n",
    "�\n",
    "2\n",
    "=\n",
    "[\n",
    "1\n",
    "1\n",
    "]\n",
    "v \n",
    "2\n",
    "​\n",
    " =[ \n",
    "1\n",
    "1\n",
    "​\n",
    " ].\n",
    "\n",
    "Now, \n",
    "�\n",
    "=\n",
    "[\n",
    "1\n",
    "1\n",
    "−\n",
    "1\n",
    "1\n",
    "]\n",
    "P=[ \n",
    "1\n",
    "−1\n",
    "​\n",
    "  \n",
    "1\n",
    "1\n",
    "​\n",
    " ] (matrix of eigenvectors) and \n",
    "�\n",
    "=\n",
    "[\n",
    "3\n",
    "0\n",
    "0\n",
    "4\n",
    "]\n",
    "D=[ \n",
    "3\n",
    "0\n",
    "​\n",
    "  \n",
    "0\n",
    "4\n",
    "​\n",
    " ] (diagonal matrix of eigenvalues).\n",
    "\n",
    "The Spectral Theorem ensures that for symmetric matrices like \n",
    "�\n",
    "A, it is always possible to find such an orthogonal matrix \n",
    "�\n",
    "P and a diagonal matrix \n",
    "�\n",
    "D such that \n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "A=PDP \n",
    "T\n",
    " ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf6690-58da-4051-9565-981af6c2374b",
   "metadata": {},
   "source": [
    "# Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd0172-e952-425a-a377-4646e5c83d2e",
   "metadata": {},
   "source": [
    "Finding Eigenvalues and their Significance:\n",
    "\n",
    "Eigenvalues are found by solving the characteristic equation \n",
    "det\n",
    "⁡\n",
    "(\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "0\n",
    "det(A−λI)=0, where \n",
    "�\n",
    "A is the matrix and \n",
    "�\n",
    "λ is a scalar (eigenvalue).\n",
    "\n",
    "Eigenvalues represent the scale factors by which the eigenvectors are stretched or shrunk when the linear transformation represented by the matrix is applied. They also provide information about the behavior of the transformation along different directions in space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55104521-81ee-43cc-af54-fcbe843e8317",
   "metadata": {},
   "source": [
    "# Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496c756-8266-4ce0-9e30-919d9acbc4c5",
   "metadata": {},
   "source": [
    ": Eigenvectors and their Relation to Eigenvalues:\n",
    "\n",
    "Eigenvectors are non-zero vectors that only change by a scalar factor when a linear transformation is applied to them. Formally, for a matrix \n",
    "�\n",
    "A and a vector \n",
    "�\n",
    "v, \n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "Av=λv where \n",
    "�\n",
    "λ is the eigenvalue.\n",
    "\n",
    "Eigenvectors are intimately related to eigenvalues. They provide the directions along which the linear transformation represented by the matrix has a simple behavior - only a scaling. The eigenvalue associated with an eigenvector determines the factor by which the eigenvector is scaled.\n",
    "\n",
    "In applications, eigenvectors can represent important structural features or patterns in the data. They are also used in various fields including physics, computer graphics, and machine learning for tasks like image processing, motion analysis, and dimensionality reduction.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea32e3-261b-496c-8257-4f365421c008",
   "metadata": {},
   "source": [
    "# Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e859a8-916d-4c57-a004-6e486caedc91",
   "metadata": {},
   "source": [
    "Geometric Interpretation of Eigenvectors and Eigenvalues\n",
    "\n",
    "Eigenvectors are special vectors associated with a linear transformation (like a matrix). When a matrix is applied to an eigenvector, the resulting vector is parallel (it may just be scaled or reversed in direction). In other words, the direction of the eigenvector doesn't change, only its magnitude (or it might flip direction if it's a negative eigenvalue).\n",
    "\n",
    "Eigenvalues are the scaling factors that apply to the eigenvectors. They represent how much the eigenvector is stretched or shrunk during the transformation.\n",
    "\n",
    "Visually, imagine a matrix transformation as warping or stretching space. The eigenvectors are the vectors that, after the transformation, remain on the same line or plane. The eigenvalues represent how much these eigenvectors are stretched or squished.\n",
    "\n",
    "For example, if you have a 2x2 matrix A and an eigenvector v, such that Av = λv, where λ is the eigenvalue, this means that when you apply A to v, you get a vector that's in the same direction as v, but scaled by a factor of λ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507255db-3a9e-4bd4-9590-e789e93a84df",
   "metadata": {},
   "source": [
    "# Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4d00c-94f5-485a-8a9c-27ef4d652901",
   "metadata": {},
   "source": [
    "Real-World Applications of Eigen Decomposition\n",
    "\n",
    "1. Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that uses eigen decomposition to find a new coordinate system where the data has maximum variance. It's widely used in fields like computer vision, data analysis, and signal processing.\n",
    "\n",
    "2. Image and Signal Processing: Eigen decomposition is used for tasks like image compression, noise reduction, and feature extraction. It's used in techniques like the Singular Value Decomposition (SVD) for these purposes.\n",
    "\n",
    "3. Quantum Mechanics: In quantum mechanics, eigenvectors and eigenvalues are fundamental concepts. They represent the states of a quantum system and the corresponding possible measurement outcomes.\n",
    "\n",
    "4. Structural Engineering: Eigen decomposition is used in the analysis of structures to find the natural frequencies and modes of vibration. This is crucial for designing buildings, bridges, and other infrastructure.\n",
    "\n",
    "5. Google's PageRank Algorithm: PageRank, the algorithm used by Google to rank web pages, is based on the concept of eigenvectors. It determines the importance of web pages by considering the link structure of the web.\n",
    "\n",
    "6. Machine Learning: Eigen decomposition is used in various machine learning algorithms, especially in techniques like Principal Component Analysis (PCA) for feature extraction and dimensionality reduction.\n",
    "\n",
    "7. Vibrations and Mechanical Systems: Eigen decomposition helps in analyzing and solving problems related to vibrations in mechanical systems, such as finding the natural frequencies and modes of vibration.\n",
    "\n",
    "These are just a few examples, and the applications of eigen decomposition are widespread across various fields of science and engineering. It's a powerful mathematical tool with broad practical implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9494b-3406-48bd-a667-dee9cf0d7cd6",
   "metadata": {},
   "source": [
    "# Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabb83b-ee0b-4d3d-abff-2bb46909dbd6",
   "metadata": {},
   "source": [
    "Can a matrix have more than one set of eigenvectors and eigenvalues?\n",
    "\n",
    "Yes, a matrix can have multiple sets of eigenvectors and eigenvalues. This happens when a matrix has repeated eigenvalues, which means that the algebraic multiplicity (the number of times an eigenvalue appears as a root of the characteristic polynomial) is greater than the geometric multiplicity (the number of linearly independent eigenvectors associated with an eigenvalue)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62a0f2-9d21-4190-add5-2970050d2144",
   "metadata": {},
   "source": [
    "# Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bca88-9052-4dfe-b7d6-ddad434594b4",
   "metadata": {},
   "source": [
    "Applications of Eigen-Decomposition in Data Analysis and Machine Learning\n",
    "\n",
    "Eigen-Decomposition is a fundamental tool in data analysis and machine learning. Here are three specific applications or techniques that rely on Eigen-Decomposition:\n",
    "\n",
    "1. Principal Component Analysis (PCA):\n",
    "\n",
    "Description: PCA is a dimensionality reduction technique that uses eigen decomposition to find a new coordinate system where the data has maximum variance.\n",
    "How It Works: PCA finds the eigenvectors and eigenvalues of the covariance matrix of the data. The eigenvectors (principal components) corresponding to the largest eigenvalues capture the most important information in the data.\n",
    "Use Case: PCA is used for tasks like image compression, face recognition, and data visualization.\n",
    "2. Singular Value Decomposition (SVD):\n",
    "\n",
    "Description: SVD is a generalization of eigen decomposition for any matrix, not just square ones. It is widely used in various applications including image processing, collaborative filtering, and data compression.\n",
    "How It Works: SVD decomposes a matrix into three matrices - U, Σ, and V. The singular values in Σ are akin to eigenvalues in eigen decomposition.\n",
    "Use Case: SVD is used in recommendation systems, image compression (JPEG), and data compression.\n",
    "3. Linear Discriminant Analysis (LDA):\n",
    "\n",
    "Description: LDA is a supervised dimensionality reduction technique used for classification. It tries to find a subspace where the classes are well separated.\n",
    "How It Works: LDA involves finding eigenvectors of the between-class scatter matrix and the within-class scatter matrix. These eigenvectors form the basis of the discriminant space.\n",
    "Use Case: LDA is used in face recognition, speech processing, and other classification tasks.\n",
    "These applications demonstrate how Eigen-Decomposition enables us to extract meaningful information from data, reduce dimensionality while preserving important features, and improve the performance of various machine learning algorithms. It's a powerful mathematical tool that finds extensive use in data-driven fields.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec239d3d-a29b-40fa-9ed9-005e7ed90f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
